import os
import streamlit as st

# ä»ç³»ç»Ÿç¯å¢ƒå˜é‡ä¸­è·å– API å¯†é’¥
os.environ["OPENAI_API_KEY"] = st.secrets["KEY"]

from llama_index import download_loader
from llama_index.node_parser import SimpleNodeParser
from llama_index import GPTSimpleVectorIndex
from llama_index import LLMPredictor, GPTSimpleVectorIndex, PromptHelper, ServiceContext
from langchain import OpenAI

#doc_path ='D://pythonsave//data'
doc_path = './data/'
index_file = 'index.json'

if 'response' not in st.session_state:
    st.session_state.response = ''

if not os.path.exists(doc_path):
    os.makedirs(doc_path)

def send_click():
    st.session_state.response  = index.query(st.session_state.prompt)

index = None
st.title("Yeyu's Doc Chat")

sidebar_placeholder = st.sidebar.container()
#uploaded_file = st.file_uploader("Choose a file")
# è®¾ç½®å¯†ç 
password = "123456"

# åœ¨ä¾§è¾¹æ æ·»åŠ ä¸€ä¸ªæ–‡æœ¬æ¡†ï¼Œç”¨äºè¾“å…¥å¯†ç 
password_placeholder = st.sidebar.empty()
password_input = password_placeholder.text_input("Enter the password", type="password")

# å£°æ˜ uploaded_file å˜é‡
uploaded_file = None

if password_input == password:
    # å¯†ç æ­£ç¡®ï¼Œå…è®¸ä¸Šä¼ æ–‡ä»¶
    uploaded_file = st.file_uploader("Choose a file")
    # å…¶ä»–æ“ä½œ...
else:
    # å¯†ç é”™è¯¯ï¼Œç¦æ­¢ä¸Šä¼ æ–‡ä»¶
    st.warning("æ‚¨å¥½ï¼Œæˆ‘æ˜¯ä¸½è¾¾å°ä¸½ï¼Œè§åˆ°ä½ å¾ˆé«˜å…´ï¼")










if uploaded_file is not None:

    doc_files = os.listdir(doc_path)
    for doc_file in doc_files:
        os.remove(doc_path  + doc_file)

    bytes_data = uploaded_file.read()
    with open(f"{doc_path}{uploaded_file.name}", 'wb') as f: 
        f.write(bytes_data)

    SimpleDirectoryReader = download_loader("SimpleDirectoryReader")

    loader = SimpleDirectoryReader(doc_path, recursive=True, exclude_hidden=True)
    documents = loader.load_data()
    sidebar_placeholder.header('Current Processing Document:')
    sidebar_placeholder.subheader(uploaded_file.name)
    sidebar_placeholder.write(documents[0].get_text()[:10000]+'...')

    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name="text-davinci-003"))

    max_input_size = 4096
    num_output = 256
    max_chunk_overlap = 20
    prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)

    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)

    index = GPTSimpleVectorIndex.from_documents(
        documents, service_context=service_context
    )

    index.save_to_disk(index_file)

elif os.path.exists(index_file):
    index = GPTSimpleVectorIndex.load_from_disk(index_file)

    SimpleDirectoryReader = download_loader("SimpleDirectoryReader")
    loader = SimpleDirectoryReader(doc_path, recursive=True, exclude_hidden=True)
    documents = loader.load_data()
    doc_filename = os.listdir(doc_path)[0]
    sidebar_placeholder.header('Current Processing Document:')
    sidebar_placeholder.subheader(doc_filename)
    sidebar_placeholder.write(documents[0].get_text()[:10000]+'...')

if index != None:
    st.text_input("Ask something: ", key='prompt', value='è¯·è¾“å…¥é—®é¢˜ï¼š')
    st.button("Send", on_click=send_click)
    if st.session_state.response:
        st.subheader("Response: ")
        st.success(st.session_state.response, icon= "ğŸ¤–")